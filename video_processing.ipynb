{"cells":[{"cell_type":"markdown","metadata":{"id":"_A0bqe2E6uBl"},"source":["## **Import required libraries**"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"E_Z0WofjlZKV"},"outputs":[],"source":["import os\n","import cv2\n","from moviepy.editor import VideoFileClip\n","import time\n","import base64\n","\n","from langchain_openai import AzureChatOpenAI\n","from langchain_core.messages import SystemMessage, HumanMessage"]},{"cell_type":"markdown","metadata":{"id":"UMyleS5T7VW7"},"source":["## **Load video and process it into frames**"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"IEp62q9_GJ9Z"},"outputs":[],"source":["def process_video(video_path, seconds_per_frame=2):\n","    base64Frames = []\n","    base_video_path, _ = os.path.splitext(video_path)\n","\n","    video = cv2.VideoCapture(video_path)\n","    total_frames = int(video.get(cv2.CAP_PROP_FRAME_COUNT))\n","    fps = video.get(cv2.CAP_PROP_FPS)\n","    frames_to_skip = int(fps * seconds_per_frame)\n","    curr_frame=0\n","\n","    # Loop through the video and extract frames at specified sampling rate\n","    while curr_frame < total_frames - 1:\n","        video.set(cv2.CAP_PROP_POS_FRAMES, curr_frame)\n","        success, frame = video.read()\n","        if not success:\n","            break\n","        _, buffer = cv2.imencode(\".jpg\", frame)\n","        base64Frames.append(base64.b64encode(buffer).decode(\"utf-8\"))\n","        curr_frame += frames_to_skip\n","    video.release()\n","\n","    print(f\"Extracted {len(base64Frames)} frames\")\n","    return base64Frames"]},{"cell_type":"markdown","metadata":{"id":"OO4p1rjXJNu5"},"source":["## **Process videos**"]},{"cell_type":"code","execution_count":null,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"elapsed":5,"status":"ok","timestamp":1729883290940,"user":{"displayName":"Amrita Thakur","userId":"18437203915913693975"},"user_tz":-345},"id":"47r5BlWbEl_0","outputId":"15150cd0-8bdc-4fc8-8aee-cb64890cd71d"},"outputs":[{"name":"stdout","output_type":"stream","text":["Extracted 4 frames\n"]}],"source":["video_path = \"path/to/video.mp4\"\n","base64Frames = process_video(video_path, seconds_per_frame=1)"]},{"cell_type":"markdown","metadata":{"id":"s4Fhu3OcI-5P"},"source":["## **Use AzureChatOpenAI for summarizing videos**\n","\n","The visual summary is generated by sending the model only the frames from the video. With just the frames, the model is likely to capture the visual aspects, but will miss any details discussed by the speaker."]},{"cell_type":"code","execution_count":null,"metadata":{"id":"-1tFRl4sGgx-"},"outputs":[],"source":["os.environ[\"AZURE_OPENAI_ENDPOINT\"] = os.getenv(\"AZURE_OPENAI_ENDPOINT\")\n","\n","model = AzureChatOpenAI(\n","    openai_api_version=os.getenv(\"AZURE_OAI_API_VERSION\"),\n","    azure_deployment=os.getenv(\"AZURE_OAI_DEPLOYMENT\"),\n","    openai_api_key=os.getenv(\"AZURE_OAI_KEY\"),\n","    openai_api_type='openai',\n","    temperature=0.0,\n","    streaming=False\n",")\n","\n","messages=[\n","    {\"role\": \"system\", \"content\": \"You are generating a video summary. Please provide a summary of the video. Respond in Markdown.\"},\n","    {\"role\": \"user\", \"content\": [\n","        {\"type\": \"text\", \"text\": \"These are the frames from the video.\"},\n","        *map(lambda x: {\"type\": \"image_url\",\n","                        \"image_url\": {\"url\": f'data:image/jpg;base64,{x}', \"detail\": \"low\"}}, base64Frames)\n","        ],\n","    }\n","    ]\n","ai_message = model.invoke(messages)\n","video_summary = ai_message.content"]},{"cell_type":"markdown","metadata":{"id":"4849O3eaKK6O"},"source":["## **Output video summary**"]},{"cell_type":"code","execution_count":null,"metadata":{"colab":{"base_uri":"https://localhost:8080/","height":122},"executionInfo":{"elapsed":461,"status":"ok","timestamp":1729883370198,"user":{"displayName":"Amrita Thakur","userId":"18437203915913693975"},"user_tz":-345},"id":"SSZgEDrGFV1_","outputId":"33c73a5d-240a-49f1-b270-00fe027e9d03"},"outputs":[{"data":{"application/vnd.google.colaboratory.intrinsic+json":{"type":"string"},"text/plain":["\"The frames from the video appear to be from an ultrasound scan. Ultrasound imaging is commonly used in medical settings to visualize internal organs, tissues, and, in some cases, developing fetuses. The images show various cross-sectional views, likely of an abdominal region or a developing fetus, though it's not entirely clear without more specific context.\\n\\n### Summary\\n- **Type of Video**: Medical ultrasound imaging\\n- **Content**: The video consists of a series of ultrasound frames showing internal anatomy.\\n- **Purpose**: Likely diagnostic, monitoring, or examination of internal structures.\\n\\nThese types of images are typically used by healthcare professionals for diagnosis or to monitor the health and development of organs or fetuses.\""]},"execution_count":20,"metadata":{},"output_type":"execute_result"}],"source":["video_summary"]}],"metadata":{"colab":{"authorship_tag":"ABX9TyPJJnHVbxz4fVtSgZyzyucY","mount_file_id":"1Ev0YsZ4LQEzd55XFLSlpRUqDjRkdWwmf","provenance":[{"file_id":"1Ev0YsZ4LQEzd55XFLSlpRUqDjRkdWwmf","timestamp":1729883424567},{"file_id":"1lCzF_zoDu4JoYxWndlazgBtkdSwJzn8f","timestamp":1729880955654},{"file_id":"1HZojnwtPhWFGMfCgGefsTyYp5hl_A1yP","timestamp":1729857445033},{"file_id":"19O5Zui6FJQ4kxhArlescsvWprnTt-8NZ","timestamp":1729790589659}]},"kernelspec":{"display_name":"Python 3","name":"python3"},"language_info":{"name":"python"}},"nbformat":4,"nbformat_minor":0}
